{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bus': 0, 'saab': 1, 'opel': 2, 'van': 3}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv('car_data.csv', delimiter=',', header=None, index_col=0).values\n",
    "classes = list(set(dataset[:, -1]))\n",
    "classMap = dict(zip(classes, range(len(classes))))\n",
    "print(classMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTwoClassMap(classMap, names):\n",
    "    if (len(names) != 2):\n",
    "        print(\"must be two classes\")\n",
    "    return dict([(names[0], -1), (names[1], 1)])\n",
    "\n",
    "def convert(x, y, classMap):\n",
    "    res_x = list()\n",
    "    res_y = list()\n",
    "    for i in range (len(y)):\n",
    "        classLabel = classMap.get(y[i])\n",
    "        if (classLabel != None):\n",
    "            res_y.append(classLabel)\n",
    "            #inserting 1 as bias\n",
    "            res_x.append(np.insert(x[i], 0, 1, axis = 0))\n",
    "    return normalize(np.array(res_x)), np.array(res_y)\n",
    "\n",
    "def normalize(x):\n",
    "    y = np.transpose(x.astype(float))\n",
    "    for i in range(y.shape[0]):\n",
    "        y[i] /= np.max(y[i])\n",
    "    return np.transpose(y)\n",
    "\n",
    "def calculateClassAm(classes, classArr):\n",
    "    ctr = np.zeros(len(classes))\n",
    "    for className in classArr:\n",
    "        for i in range(len(classes)):\n",
    "            if (classes[i] == className):\n",
    "                ctr[i] = ctr[i] + 1\n",
    "    for i in range(len(classes)):\n",
    "        print(\"\\tclass \", classes[i], \" : \", ctr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tclass  bus  :  218.0\n",
      "\tclass  saab  :  217.0\n",
      "\tclass  opel  :  212.0\n",
      "\tclass  van  :  199.0\n"
     ]
    }
   ],
   "source": [
    "calculateClassAm(classes, dataset[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(846,)\n"
     ]
    }
   ],
   "source": [
    "print(dataset[:,-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with  {'bus': -1, 'saab': 1}\n",
      "data shape  (435, 19)\n",
      "labels shape  (435,)\n",
      "x_train shape  (348, 19)\n",
      "x_test shape  (87, 19)\n"
     ]
    }
   ],
   "source": [
    "#will working with bus and van\n",
    "workingPair = classes[:2]\n",
    "twoClassMap = getTwoClassMap(classMap, workingPair)\n",
    "print(\"working with \", twoClassMap)\n",
    "\n",
    "data, labels = convert(dataset[:, :-1].astype(int), dataset[:, -1], twoClassMap)\n",
    "print(\"data shape \", data.shape)\n",
    "print(\"labels shape \", labels.shape)\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n",
    "print(\"x_train shape \", x_train.shape)\n",
    "print(\"x_test shape \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1. + math.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return math.exp(-x) / (1. + math.exp(-x))**2\n",
    "\n",
    "def dsigmoid_opt(sigmoid_value):\n",
    "    return sigmoid_value* (1 - sigmoid_value)\n",
    "\n",
    "def margin(x,y,w):\n",
    "    return y*x.dot(w)\n",
    "    \n",
    "def logloss(x,y,w):\n",
    "    res = 0\n",
    "    for i in range(len(y)):\n",
    "        res = res + math.log(sigmoid(margin(x[i], y[i], w)))\n",
    "    return res\n",
    "\n",
    "def dlogloss(x,y,w):\n",
    "    res = np.zeros(w.shape)\n",
    "    for i in range(len(w)):\n",
    "        for j in range(len(y)):\n",
    "            sigma = sigmoid(margin(x[j], y[j], w))\n",
    "            res[i] = res[i] + dsigmoid_opt(sigma) / sigma * y[j] * x[j][i]\n",
    "    return res\n",
    "            \n",
    "def logReg(x, y, eps, step, w0, changeStep):\n",
    "    w = np.zeros(x.shape[1])\n",
    "    w[0] = w0\n",
    "    prev_w = np.zeros(x.shape[1])\n",
    "    diff_norm = eps\n",
    "    i = 0\n",
    "    while (diff_norm >= eps):\n",
    "        if (changeStep):\n",
    "            step = step / (i + 1)**0.3\n",
    "        prev_w = w\n",
    "        w = w + step * dlogloss(x,y,w)\n",
    "        log_loss = logloss(x,y,w)\n",
    "        diff_norm = np.linalg.norm(w - prev_w)\n",
    "        if (i % 100 == 0):\n",
    "            print(\"iter \", i, \" log loss = \", log_loss, \" diff_norm = \", diff_norm)\n",
    "        i = i + 1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  0  log loss =  -244.9898283309022  diff_norm =  0.2581604237245165\n",
      "iter  100  log loss =  -202.89857181501597  diff_norm =  0.019663793650147407\n",
      "iter  200  log loss =  -189.46064745084067  diff_norm =  0.014003790238188325\n",
      "iter  300  log loss =  -181.42685105856208  diff_norm =  0.011624004623929976\n",
      "iter  400  log loss =  -175.47059282120762  diff_norm =  0.010318853120535562\n",
      "iter  500  log loss =  -170.60731260555573  diff_norm =  0.009462208017126102\n",
      "iter  600  log loss =  -166.44085078213038  diff_norm =  0.008827447989698037\n",
      "iter  700  log loss =  -162.77481519412848  diff_norm =  0.008319661229328356\n",
      "iter  800  log loss =  -159.49526824546382  diff_norm =  0.007893730581161265\n",
      "iter  900  log loss =  -156.52809320028413  diff_norm =  0.007525486000667204\n",
      "iter  1000  log loss =  -153.82107876183105  diff_norm =  0.007200537048060123\n",
      "iter  1100  log loss =  -151.33536406030376  diff_norm =  0.006909558286856618\n",
      "iter  1200  log loss =  -149.04086994294696  diff_norm =  0.006646111905716147\n",
      "iter  1300  log loss =  -146.9136241304466  diff_norm =  0.0064055438638314696\n",
      "iter  1400  log loss =  -144.93408016466603  diff_norm =  0.006184371635179546\n",
      "iter  1500  log loss =  -143.08600366132836  diff_norm =  0.005979917110874697\n",
      "iter  1600  log loss =  -141.35570463186227  diff_norm =  0.005790072671846978\n",
      "iter  1700  log loss =  -139.73149172210097  diff_norm =  0.0056131453737803495\n",
      "iter  1800  log loss =  -138.2032741561106  diff_norm =  0.005447749867266052\n",
      "iter  1900  log loss =  -136.76226484858705  diff_norm =  0.0052927331388570075\n",
      "iter  2000  log loss =  -135.40075444731053  diff_norm =  0.005147120700507272\n",
      "iter  2100  log loss =  -134.11193612329362  diff_norm =  0.005010077557852016\n",
      "iter  2200  log loss =  -132.88976734947616  diff_norm =  0.0048808795233936785\n",
      "iter  2300  log loss =  -131.72885911848272  diff_norm =  0.004758891858869678\n",
      "iter  2400  log loss =  -130.6243858640023  diff_norm =  0.004643553163284727\n",
      "iter  2500  log loss =  -129.57201126198407  diff_norm =  0.004534363050792139\n",
      "iter  2600  log loss =  -128.56782640478946  diff_norm =  0.00443087259230405\n",
      "iter  2700  log loss =  -127.60829776052836  diff_norm =  0.004332676792251691\n",
      "iter  2800  log loss =  -126.69022297937525  diff_norm =  0.00423940857961871\n",
      "iter  2900  log loss =  -125.81069307356917  diff_norm =  0.0041507339383674005\n",
      "iter  3000  log loss =  -124.96705983484  diff_norm =  0.004066347905511374\n"
     ]
    }
   ],
   "source": [
    "w_res = logReg(x_train,y_train, 0.004, 0.002, 0.5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceOnLabels(y):\n",
    "    res = np.zeros(len(y), dtype = int)\n",
    "    for i in range(len(y)):\n",
    "        if (y[i] < 0):\n",
    "            res[i] = -1\n",
    "        else:\n",
    "            res[i] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result =  [ 0.57370236  3.46819139 -2.74040866 11.77392062  0.78731224 -8.84756256\n",
      "  6.62511999  0.03649157  4.20556388  1.90305374 -0.96290196 -4.30596082\n",
      " -1.54153713 -3.07380966 -9.05194546  2.67050111  1.10482283 -2.9584885\n",
      "  2.52470713]\n"
     ]
    }
   ],
   "source": [
    "result = replaceOnLabels(x_test.dot(w_res))\n",
    "print(\"result = \", w_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(res, real):\n",
    "    correct = 0.\n",
    "    for i in range (len(res)):\n",
    "        if (res[i] == real[i]):\n",
    "            correct = correct + 1\n",
    "    return correct / len(res)\n",
    "\n",
    "def getClassIndex(value, classes):\n",
    "    for classIndex in range(len(classes)):\n",
    "        if (classes[classIndex] == value):\n",
    "            return classIndex\n",
    "    print(\"class \", value, \" not found\")\n",
    "    return -1\n",
    "    \n",
    "def confusionMatrix(res, real, classes):\n",
    "    classAm = len(classes)\n",
    "    matrix = np.zeros(classAm * classAm).reshape(classAm, classAm)\n",
    "    for i in range(len(res)):\n",
    "        classIndexRes = getClassIndex(res[i], classes)\n",
    "        if (res[i] == real[i]):\n",
    "            matrix[classIndexRes][classIndexRes] = matrix[classIndexRes][classIndexRes] + 1\n",
    "        else:\n",
    "            classIndexReal = getClassIndex(real[i], classes)\n",
    "            matrix[classIndexRes][classIndexReal] = matrix[classIndexRes][classIndexReal] + 1\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def precision(matrix, classIndex):\n",
    "    return matrix[classIndex][classIndex] / np.sum(matrix, axis = 1)[classIndex]\n",
    "\n",
    "def recall(matrix, classIndex):\n",
    "    return matrix[classIndex][classIndex] / np.sum(matrix, axis = 0)[classIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9310344827586207\n",
      "classes are:  {'bus': -1, 'saab': 1}\n",
      "confusion Matrix:\n",
      " [[39.  4.]\n",
      " [ 2. 42.]]\n",
      "precision:  0.9545454545454546\n",
      "recall:  0.9130434782608695\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy: \", accuracy(result, y_test))\n",
    "print(\"classes are: \", twoClassMap)\n",
    "classValues = np.fromiter(twoClassMap.values(), dtype=int)\n",
    "confusion_matrix = confusionMatrix(result,y_test, classValues)\n",
    "print(\"confusion Matrix:\\n\",  confusion_matrix)\n",
    "print(\"precision: \", precision(confusion_matrix, classValues[0]))\n",
    "print(\"recall: \", recall(confusion_matrix, classValues[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classification: \n",
    "### part 1. One vs Rest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOneVsRestMap(classMap, targetClass):\n",
    "    result = dict()\n",
    "    for className in classMap.keys():\n",
    "        if (className == targetClass):\n",
    "            result[className] = 1\n",
    "        else:\n",
    "            result[className] = -1\n",
    "    return result\n",
    "            \n",
    "def convertClassNamesToLabels(classArr, classMap):\n",
    "    res = np.zeros(classArr.shape)\n",
    "    for i in range(len(classArr)):\n",
    "        res[i] = classMap.get(classArr[i])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================== \n",
      "\n",
      "working with class  bus\n",
      "x_train shape : (676, 19)  y_train shape:  (676,)\n",
      "iter  0  log loss =  -639.639104117179  diff_norm =  1.4534349753357647\n",
      "iter  100  log loss =  -335.6636069842865  diff_norm =  0.25630884697082035\n",
      "iter  200  log loss =  -306.60175022093273  diff_norm =  0.02194139623777289\n",
      "iter  300  log loss =  -290.97444653398446  diff_norm =  0.016653537060933477\n",
      "iter  400  log loss =  -278.41297403824166  diff_norm =  0.015128806570280338\n",
      "iter  500  log loss =  -267.8972998875237  diff_norm =  0.013925808115731474\n",
      "iter  600  log loss =  -258.9097113966373  diff_norm =  0.012925549088774758\n",
      "iter  700  log loss =  -251.11638848961178  diff_norm =  0.01207315208030654\n",
      "iter  800  log loss =  -244.28103250669017  diff_norm =  0.011335362749995453\n",
      "iter  900  log loss =  -238.22863803379238  diff_norm =  0.01068925966660939\n",
      "iter  1000  log loss =  -232.82579642342336  diff_norm =  0.010118109540980396\n",
      "iter  1100  log loss =  -227.96853595003256  diff_norm =  0.009609291718358948\n",
      "iter  1200  log loss =  -223.5743420240242  diff_norm =  0.009153050707469341\n",
      "iter  1300  log loss =  -219.5767234404978  diff_norm =  0.008741684797340879\n",
      "iter  1400  log loss =  -215.92140495923974  diff_norm =  0.008368999573841105\n",
      "iter  1500  log loss =  -212.56359214454258  diff_norm =  0.008029931666070234\n",
      "iter  1600  log loss =  -209.46596220114134  diff_norm =  0.007720284491496516\n",
      "iter  1700  log loss =  -206.5971585552489  diff_norm =  0.0074365388262287325\n",
      "iter  1800  log loss =  -203.93064305316034  diff_norm =  0.007175714132732983\n",
      "iter  1900  log loss =  -201.4438074361974  diff_norm =  0.006935264912633732\n",
      "iter  2000  log loss =  -199.1172763993196  diff_norm =  0.0067130016889295615\n",
      "iter  2100  log loss =  -196.9343546274265  diff_norm =  0.006507029655939598\n",
      "iter  2200  log loss =  -194.88058365582214  diff_norm =  0.006315700260871458\n",
      "iter  2300  log loss =  -192.94338360340743  diff_norm =  0.006137572437690385\n",
      "iter  2400  log loss =  -191.11176125022652  diff_norm =  0.005971381179653137\n",
      "iter  2500  log loss =  -189.3760704996969  diff_norm =  0.0058160117865071895\n",
      "iter  2600  log loss =  -187.72781457231684  diff_norm =  0.005670478566585575\n",
      "iter  2700  log loss =  -186.15948170839135  diff_norm =  0.0055339070832442945\n",
      "iter  2800  log loss =  -184.66440796954925  diff_norm =  0.00540551925419903\n",
      "iter  2900  log loss =  -183.23666209700252  diff_norm =  0.005284620770477824\n",
      "iter  3000  log loss =  -181.87094842891793  diff_norm =  0.0051705904179027385\n",
      "iter  3100  log loss =  -180.56252468461136  diff_norm =  0.005062870970848347\n",
      "iter  3200  log loss =  -179.30713204966196  diff_norm =  0.004960961393887068\n",
      "iter  3300  log loss =  -178.10093548723876  diff_norm =  0.004864410137714126\n",
      "iter  3400  log loss =  -176.94047258882915  diff_norm =  0.004772809355292796\n",
      "iter  3500  log loss =  -175.822609585905  diff_norm =  0.004685789895419446\n",
      "iter  3600  log loss =  -174.74450339066172  diff_norm =  0.004603016955754286\n",
      "iter  3700  log loss =  -173.70356873224446  diff_norm =  0.004524186297404657\n",
      "iter  3800  log loss =  -172.69744961514866  diff_norm =  0.0044490209393358605\n",
      "iter  3900  log loss =  -171.7239944566586  diff_norm =  0.004377268264155644\n",
      "iter  4000  log loss =  -170.7812343663568  diff_norm =  0.0043086974775840324\n",
      "iter  4100  log loss =  -169.86736411777426  diff_norm =  0.004243097372991251\n",
      "iter  4200  log loss =  -168.98072543380457  diff_norm =  0.004180274359710168\n",
      "iter  4300  log loss =  -168.11979226662996  diff_norm =  0.004120050720073841\n",
      "iter  4400  log loss =  -167.28315780189402  diff_norm =  0.004062263065247217\n",
      "iter  4500  log loss =  -166.46952295762864  diff_norm =  0.004006760964292995\n",
      "======================== \n",
      "\n",
      "working with class  saab\n",
      "x_train shape : (676, 19)  y_train shape:  (676,)\n",
      "iter  0  log loss =  -655.046952230792  diff_norm =  1.3763114669509766\n",
      "iter  100  log loss =  -390.34533723754765  diff_norm =  0.4931010759229416\n",
      "iter  200  log loss =  -363.92612944546033  diff_norm =  0.34998536176340345\n",
      "iter  300  log loss =  -352.50231867022757  diff_norm =  0.2715537229769269\n",
      "iter  400  log loss =  -345.4912118493452  diff_norm =  0.21667900623058203\n",
      "iter  500  log loss =  -340.2320717598539  diff_norm =  0.16566313050179612\n",
      "iter  600  log loss =  -336.0974125597944  diff_norm =  0.11169767588365125\n",
      "iter  700  log loss =  -333.0829391872676  diff_norm =  0.057281585396645456\n",
      "iter  800  log loss =  -331.1111669862974  diff_norm =  0.01825401876279891\n",
      "iter  900  log loss =  -329.6337216225973  diff_norm =  0.005881103475431287\n",
      "iter  1000  log loss =  -328.31294467955223  diff_norm =  0.005042719975501318\n",
      "iter  1100  log loss =  -327.08818510827706  diff_norm =  0.004866898874568861\n",
      "iter  1200  log loss =  -325.9403147839335  diff_norm =  0.004721490878262311\n",
      "iter  1300  log loss =  -324.85617869350875  diff_norm =  0.004595906226594322\n",
      "iter  1400  log loss =  -323.82613915096505  diff_norm =  0.004485322646840345\n",
      "iter  1500  log loss =  -322.8429890705236  diff_norm =  0.004386261108010315\n",
      "iter  1600  log loss =  -321.9012226551775  diff_norm =  0.00429620109756501\n",
      "iter  1700  log loss =  -320.99653931873416  diff_norm =  0.004213307497040769\n",
      "iter  1800  log loss =  -320.1255034022002  diff_norm =  0.004136234956165609\n",
      "iter  1900  log loss =  -319.28530882330875  diff_norm =  0.004063988421229033\n",
      "======================== \n",
      "\n",
      "working with class  opel\n",
      "x_train shape : (676, 19)  y_train shape:  (676,)\n",
      "iter  0  log loss =  -641.9632692407523  diff_norm =  1.4542157078001463\n",
      "iter  100  log loss =  -364.89534283010767  diff_norm =  0.3355357993277352\n",
      "iter  200  log loss =  -356.1410099082498  diff_norm =  0.24037234809899893\n",
      "iter  300  log loss =  -352.19466221084906  diff_norm =  0.20588922238274873\n",
      "iter  400  log loss =  -349.4243068758004  diff_norm =  0.18841413003984966\n",
      "iter  500  log loss =  -347.0324220867664  diff_norm =  0.17426861882216882\n",
      "iter  600  log loss =  -344.82333975787634  diff_norm =  0.15998948156962903\n",
      "iter  700  log loss =  -342.73886391756974  diff_norm =  0.14464787675396665\n",
      "iter  800  log loss =  -340.75909002028874  diff_norm =  0.1279114401297854\n",
      "iter  900  log loss =  -338.8796162001845  diff_norm =  0.10958377187495995\n",
      "iter  1000  log loss =  -337.1076097536669  diff_norm =  0.08957079633451219\n",
      "iter  1100  log loss =  -335.46302185575286  diff_norm =  0.0681164246890076\n",
      "iter  1200  log loss =  -333.97555743971526  diff_norm =  0.046358568201794265\n",
      "iter  1300  log loss =  -332.66339833250873  diff_norm =  0.026891869208083282\n",
      "iter  1400  log loss =  -331.5011935520601  diff_norm =  0.013016632701498006\n",
      "iter  1500  log loss =  -330.43152654682854  diff_norm =  0.006265662866998818\n",
      "iter  1600  log loss =  -329.41336728657103  diff_norm =  0.004616340049301419\n",
      "iter  1700  log loss =  -328.43234564514967  diff_norm =  0.004397896772636609\n",
      "iter  1800  log loss =  -327.4844345912046  diff_norm =  0.004318326401459549\n",
      "iter  1900  log loss =  -326.56752499746096  diff_norm =  0.004247899799437115\n",
      "iter  2000  log loss =  -325.67985527216604  diff_norm =  0.004180424935497525\n",
      "iter  2100  log loss =  -324.819842787751  diff_norm =  0.0041155086792621885\n",
      "iter  2200  log loss =  -323.9860499076742  diff_norm =  0.00405294299945454\n",
      "======================== \n",
      "\n",
      "working with class  van\n",
      "x_train shape : (676, 19)  y_train shape:  (676,)\n",
      "iter  0  log loss =  -616.5718921576953  diff_norm =  1.5309034974033864\n",
      "iter  100  log loss =  -279.51370902347594  diff_norm =  0.02327098874432735\n",
      "iter  200  log loss =  -260.1737081211433  diff_norm =  0.017423379997614924\n",
      "iter  300  log loss =  -246.8061354651701  diff_norm =  0.015513482576107953\n",
      "iter  400  log loss =  -235.7048113914373  diff_norm =  0.014360223028076931\n",
      "iter  500  log loss =  -226.05491914686442  diff_norm =  0.01346116621967935\n",
      "iter  600  log loss =  -217.52021873837987  diff_norm =  0.01269389515425474\n",
      "iter  700  log loss =  -209.90043280522815  diff_norm =  0.012015709937012743\n",
      "iter  800  log loss =  -203.05281162263122  diff_norm =  0.011406462263571131\n",
      "iter  900  log loss =  -196.86678995744037  diff_norm =  0.010854209712961175\n",
      "iter  1000  log loss =  -191.25313211624143  diff_norm =  0.010350715852784083\n",
      "iter  1100  log loss =  -186.1381512505223  diff_norm =  0.00988974262320994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  1200  log loss =  -181.4601247568792  diff_norm =  0.00946628814614206\n",
      "iter  1300  log loss =  -177.1668382093123  diff_norm =  0.009076197461520793\n",
      "iter  1400  log loss =  -173.21379068114317  diff_norm =  0.00871594048368558\n",
      "iter  1500  log loss =  -169.562827073502  diff_norm =  0.008382473205775759\n",
      "iter  1600  log loss =  -166.18106610518075  diff_norm =  0.008073144070744453\n",
      "iter  1700  log loss =  -163.04004354886445  diff_norm =  0.0077856267263293295\n",
      "iter  1800  log loss =  -160.11501789353386  diff_norm =  0.00751786924890962\n",
      "iter  1900  log loss =  -157.3844017736331  diff_norm =  0.007268054288408996\n",
      "iter  2000  log loss =  -154.82929262088936  diff_norm =  0.007034566859524169\n",
      "iter  2100  log loss =  -152.43308269243244  diff_norm =  0.006815967744399386\n",
      "iter  2200  log loss =  -150.1811332744723  diff_norm =  0.006610971177922744\n",
      "iter  2300  log loss =  -148.06050120547692  diff_norm =  0.006418425906017261\n",
      "iter  2400  log loss =  -146.05970834243993  diff_norm =  0.006237298966855158\n",
      "iter  2500  log loss =  -144.16854647447667  diff_norm =  0.006066661712333141\n",
      "iter  2600  log loss =  -142.37791163974865  diff_norm =  0.005905677699669386\n",
      "iter  2700  log loss =  -140.67966293798347  diff_norm =  0.0057535921614815105\n",
      "iter  2800  log loss =  -139.06650182997944  diff_norm =  0.005609722819465899\n",
      "iter  2900  log loss =  -137.5318686332907  diff_norm =  0.00547345184913315\n",
      "iter  3000  log loss =  -136.0698535005303  diff_norm =  0.00534421883562422\n",
      "iter  3100  log loss =  -134.67511963378647  diff_norm =  0.005221514586157072\n",
      "iter  3200  log loss =  -133.34283686855267  diff_norm =  0.005104875685228518\n",
      "iter  3300  log loss =  -132.06862407099524  diff_norm =  0.004993879695402908\n",
      "iter  3400  log loss =  -130.84849904708747  diff_norm =  0.004888140920489799\n",
      "iter  3500  log loss =  -129.67883487188345  diff_norm =  0.004787306659433569\n",
      "iter  3600  log loss =  -128.55632172051787  diff_norm =  0.004691053889130396\n",
      "iter  3700  log loss =  -127.47793342618586  diff_norm =  0.004599086322681313\n",
      "iter  3800  log loss =  -126.44089810978333  diff_norm =  0.0045111317967278875\n",
      "iter  3900  log loss =  -125.44267232547773  diff_norm =  0.004426939947641843\n",
      "iter  4000  log loss =  -124.48091824971257  diff_norm =  0.004346280141564107\n",
      "iter  4100  log loss =  -123.55348351092728  diff_norm =  0.004268939627860005\n",
      "iter  4200  log loss =  -122.65838331590757  diff_norm =  0.0041947218894298035\n",
      "iter  4300  log loss =  -121.79378457806504  diff_norm =  0.004123445166728398\n",
      "iter  4400  log loss =  -120.95799179465584  diff_norm =  0.004054941135266232\n"
     ]
    }
   ],
   "source": [
    "allData = convert(dataset[:, :-1].astype(int), dataset[:, -1], classMap)[0]\n",
    "x_train_total, x_test_total, y_train_total, y_test_total = train_test_split(allData, dataset[:,-1], test_size=0.2)\n",
    "\n",
    "y_test_arr = np.zeros(len(classes), dtype = \"object\")\n",
    "y_train_arr = np.zeros(len(classes), dtype = \"object\")\n",
    "w_arr = np.zeros(len(classes), dtype = \"object\")\n",
    "\n",
    "ctr = 0\n",
    "for className in classes:\n",
    "    y_train_arr[ctr] = convertClassNamesToLabels(y_train_total, getOneVsRestMap(classMap, className))\n",
    "    y_test_arr[ctr] = convertClassNamesToLabels(y_test_total, getOneVsRestMap(classMap, className))\n",
    "    print(\"======================== \\n\")\n",
    "    print(\"working with class \", className)\n",
    "    print(\"x_train shape :\", x_train_total.shape, \" y_train shape: \", y_train_arr[ctr].shape)\n",
    "    w_arr[ctr] = logReg(x_train_total, y_train_arr[ctr], 0.004, 0.002, 0.5, False)\n",
    "    ctr = ctr + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "for class  bus\n",
      "w =  [  3.27644107  -5.96254188   0.85025349 -18.22905206  -1.40870678\n",
      "  13.51309915 -12.38802954   2.1687746  -12.61859978  -1.71276931\n",
      "  -2.73593528   8.0717905    1.13570163   5.14068189  12.2160373\n",
      "  -2.29402227  -1.02648771   7.82267027  -2.20697906]\n",
      "accuracy =  0.888235294117647\n",
      "confusion matrix:\n",
      " [[ 35.   6.]\n",
      " [ 13. 116.]]\n",
      "precision:  0.8992248062015504\n",
      "recall:  0.9508196721311475\n",
      "\n",
      "for class  saab\n",
      "w =  [ 0.82673173  3.58904173 -4.36550587  3.60599632  3.52968832 -3.50316712\n",
      "  0.48307152  0.79987199 -0.4584361   1.40124498 -5.52732913 -0.71438018\n",
      " -0.73733954  1.7236649  -4.97993285  1.20772771  0.88627629 -1.32002469\n",
      "  1.6346872 ]\n",
      "accuracy =  0.7941176470588235\n",
      "confusion matrix:\n",
      " [[  9.   5.]\n",
      " [ 30. 126.]]\n",
      "precision:  0.8076923076923077\n",
      "recall:  0.9618320610687023\n",
      "\n",
      "for class  opel\n",
      "w =  [ 2.07738839 -7.77234584  1.9493152   2.76149793  2.32708378 -3.94179582\n",
      "  0.11374353  2.49446144  0.61739985  1.70361887 -0.53367401 -1.29449219\n",
      "  2.0137379  -3.72388851 -3.76566363  0.72376986  0.46171483 -0.11975405\n",
      "  1.23649364]\n",
      "accuracy =  0.7411764705882353\n",
      "confusion matrix:\n",
      " [[  5.   2.]\n",
      " [ 42. 121.]]\n",
      "precision:  0.7423312883435583\n",
      "recall:  0.983739837398374\n",
      "\n",
      "for class  van\n",
      "w =  [ -5.28519003   9.3494239    5.36233337  12.02035763  -7.9745154\n",
      "  -1.09891303  10.916537   -10.12731332   5.4731183   -7.4644178\n",
      "  16.57375184 -10.49888476 -12.19735777  -4.08815146  -5.42278033\n",
      "  -0.85788342  -1.70827981  -3.80951196   1.27082861]\n",
      "accuracy =  0.9470588235294117\n",
      "confusion matrix:\n",
      " [[ 32.   5.]\n",
      " [  4. 129.]]\n",
      "precision:  0.9699248120300752\n",
      "recall:  0.9626865671641791\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(classes)):\n",
    "    print(\"\\nfor class \", classes[i])\n",
    "    print(\"w = \", w_arr[i])\n",
    "    res = replaceOnLabels(x_test_total.dot(w_arr[i]))\n",
    "    print(\"accuracy = \", accuracy(res, y_test_arr[i]))\n",
    "    confusion_matrix_one_vs_rest = confusionMatrix(res, y_test_arr[i], np.array([1, -1]))\n",
    "    print(\"confusion matrix:\\n\", confusion_matrix_one_vs_rest)\n",
    "    print(\"precision: \", precision(confusion_matrix_one_vs_rest, 1))\n",
    "    print(\"recall: \", recall(confusion_matrix_one_vs_rest, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResultClass(classes, w_arr, x_test):\n",
    "    result = np.zeros(x_test.shape[0], dtype = \"object\")\n",
    "    classProbability = x_test.dot(w_arr[0])\n",
    "    for i in range(1, len(classes)):\n",
    "        classProbability = np.vstack((classProbability, x_test.dot(w_arr[i])))\n",
    "    classProbability = classProbability.T\n",
    "    print(classProbability.shape)\n",
    "    for i in range(result.shape[0]):\n",
    "        bestClass = 0\n",
    "        for j in range(len(classes)):\n",
    "            if (j == 0):\n",
    "                maxProb = sigmoid(classProbability[i][j])\n",
    "            if (sigmoid(classProbability[i][j]) > maxProb):\n",
    "                maxProb = sigmoid(classProbability[i][j])\n",
    "                bestClass = j\n",
    "        result[i] = classes[bestClass]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170, 4)\n",
      "accuracy =  0.7058823529411765\n",
      "distribution of classes in the sample \n",
      "\tclass  bus  :  48.0\n",
      "\tclass  saab  :  39.0\n",
      "\tclass  opel  :  47.0\n",
      "\tclass  van  :  36.0\n",
      "confusion matrix for classes  ['bus', 'saab', 'opel', 'van']  is:\n",
      " [[43.  7.  3.  2.]\n",
      " [ 2. 26. 20.  0.]\n",
      " [ 1.  3. 17.  0.]\n",
      " [ 2.  3.  7. 34.]]\n",
      "for class  bus\n",
      "precision :  0.7818181818181819\n",
      "recall :  0.8958333333333334\n",
      "for class  saab\n",
      "precision :  0.5416666666666666\n",
      "recall :  0.6666666666666666\n",
      "for class  opel\n",
      "precision :  0.8095238095238095\n",
      "recall :  0.3617021276595745\n",
      "for class  van\n",
      "precision :  0.7391304347826086\n",
      "recall :  0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "oneVsRestResult = getResultClass(classes, w_arr, x_test_total)\n",
    "print(\"accuracy = \", accuracy(oneVsRestResult, y_test_total))\n",
    "print(\"distribution of classes in the sample \")\n",
    "calculateClassAm(classes, y_test_total)\n",
    "final_oneVsRest_confusionMatrix = confusionMatrix(oneVsRestResult, y_test_total, classes)\n",
    "print(\"confusion matrix for classes \", classes, \" is:\\n\", final_oneVsRest_confusionMatrix)\n",
    "for i in range(len(classes)):\n",
    "    print(\"for class \", classes[i])\n",
    "    print(\"precision : \", precision(final_oneVsRest_confusionMatrix, i))\n",
    "    print(\"recall : \", recall(final_oneVsRest_confusionMatrix, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 2. One vs One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bus': -1, 'saab': 1},\n",
       " {'saab': -1, 'opel': 1},\n",
       " {'opel': -1, 'van': 1},\n",
       " {'bus': -1, 'opel': 1},\n",
       " {'bus': -1, 'van': 1},\n",
       " {'saab': -1, 'van': 1}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneVsOneMaps = [\n",
    "    getTwoClassMap(classMap, classes[:2]),\n",
    "    getTwoClassMap(classMap, classes[1:3]),\n",
    "    getTwoClassMap(classMap, classes[2:4]),\n",
    "    getTwoClassMap(classMap, [classes[0],classes[2]]),\n",
    "    getTwoClassMap(classMap, [classes[0],classes[3]]),\n",
    "    getTwoClassMap(classMap, [classes[1],classes[3]])\n",
    "               ]\n",
    "oneVsOneMaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertForOneToOne(x, y, classMap):\n",
    "    res_x = list()\n",
    "    res_y = list()\n",
    "    for i in range(len(y)):\n",
    "        if (classMap.get(y[i]) != None):\n",
    "            res_x.append(x[i])\n",
    "            res_y.append(classMap.get(y[i]))\n",
    "        \n",
    "    return normalize(np.array(res_x)), np.array(res_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================\n",
      "classes are:  {'bus': -1, 'saab': 1}\n",
      "iter  0  log loss =  -220.34021433763638  diff_norm =  0.2338683472301085\n",
      "iter  100  log loss =  -181.54334993761555  diff_norm =  0.019604807798785904\n",
      "iter  200  log loss =  -168.3858522632286  diff_norm =  0.013656166277529313\n",
      "iter  300  log loss =  -160.97735919441791  diff_norm =  0.010980451622333597\n",
      "iter  400  log loss =  -155.79351512982765  diff_norm =  0.00951665582853132\n",
      "iter  500  log loss =  -151.72074686175114  diff_norm =  0.008602588098606164\n",
      "iter  600  log loss =  -148.30419549081876  diff_norm =  0.007969244917344867\n",
      "iter  700  log loss =  -145.32504856058074  diff_norm =  0.007493369424876624\n",
      "iter  800  log loss =  -142.66416520093492  diff_norm =  0.007113424488019891\n",
      "iter  900  log loss =  -140.249873104858  diff_norm =  0.006796320591575251\n",
      "iter  1000  log loss =  -138.03541849735248  diff_norm =  0.006523023476697676\n",
      "iter  1100  log loss =  -135.9882448776391  diff_norm =  0.006281896186884742\n",
      "iter  1200  log loss =  -134.08446589438725  diff_norm =  0.006065431444277468\n",
      "iter  1300  log loss =  -132.3058088226724  diff_norm =  0.005868556611502355\n",
      "iter  1400  log loss =  -130.63781802579024  diff_norm =  0.005687706580961024\n",
      "iter  1500  log loss =  -129.0687421683671  diff_norm =  0.005520290495785491\n",
      "iter  1600  log loss =  -127.58881357564674  diff_norm =  0.005364370346548248\n",
      "iter  1700  log loss =  -126.18976367635493  diff_norm =  0.005218459010540827\n",
      "iter  1800  log loss =  -124.86448662767789  diff_norm =  0.005081388708185221\n",
      "iter  1900  log loss =  -123.60679928770375  diff_norm =  0.004952222726625577\n",
      "iter  2000  log loss =  -122.41126571569072  diff_norm =  0.004830194720196369\n",
      "iter  2100  log loss =  -121.27306598508468  diff_norm =  0.004714666142820865\n",
      "iter  2200  log loss =  -120.1878960917094  diff_norm =  0.004605095910487784\n",
      "iter  2300  log loss =  -119.15189010546368  diff_norm =  0.004501018483157128\n",
      "iter  2400  log loss =  -118.16155851767712  diff_norm =  0.004402027837114192\n",
      "iter  2500  log loss =  -117.21373858063689  diff_norm =  0.004307765611522759\n",
      "iter  2600  log loss =  -116.3055536732501  diff_norm =  0.004217912243710464\n",
      "iter  2700  log loss =  -115.4343795707644  diff_norm =  0.004132180263096686\n",
      "iter  2800  log loss =  -114.59781608005284  diff_norm =  0.004050309156243313\n",
      "iter  2900  log loss =  -113.79366291039597  diff_norm =  0.003972061383750207\n",
      "iter  3000  log loss =  -113.01989893864821  diff_norm =  0.003897219247812391\n",
      "iter  3100  log loss =  -112.27466423422192  diff_norm =  0.0038255823928178747\n",
      "iter  3200  log loss =  -111.55624435847616  diff_norm =  0.003756965781070905\n",
      "iter  3300  log loss =  -110.86305656191838  diff_norm =  0.0036911980284404684\n",
      "iter  3400  log loss =  -110.1936375828828  diff_norm =  0.003628120015548322\n",
      "iter  3500  log loss =  -109.54663281122023  diff_norm =  0.0035675837123767453\n",
      "iter  3600  log loss =  -108.92078662572598  diff_norm =  0.0035094511702825106\n",
      "iter  3700  log loss =  -108.31493374858506  diff_norm =  0.0034535936471623984\n",
      "iter  3800  log loss =  -107.72799148687345  diff_norm =  0.003399890840025901\n",
      "iter  3900  log loss =  -107.15895275214628  diff_norm =  0.0033482302055094556\n",
      "iter  4000  log loss =  -106.60687976584992  diff_norm =  0.003298506353447745\n",
      "iter  4100  log loss =  -106.07089837174347  diff_norm =  0.0032506205019733375\n",
      "iter  4200  log loss =  -105.55019288748616  diff_norm =  0.0032044799851449843\n",
      "iter  4300  log loss =  -105.04400143660358  diff_norm =  0.0031599978059601765\n",
      "iter  4400  log loss =  -104.55161170959472  diff_norm =  0.0031170922289727367\n",
      "iter  4500  log loss =  -104.07235710930537  diff_norm =  0.0030756864078608647\n",
      "iter  4600  log loss =  -103.60561324109685  diff_norm =  0.003035708044002261\n",
      "accuracy:  0.8611111111111112\n",
      "confusion Matrix:\n",
      " [[14.  4.]\n",
      " [ 1. 17.]]\n",
      "precision:  0.9444444444444444\n",
      "recall:  0.8095238095238095\n",
      "\n",
      "==================\n",
      "classes are:  {'saab': -1, 'opel': 1}\n",
      "iter  0  log loss =  -222.5879473526811  diff_norm =  0.30376470987747856\n",
      "iter  100  log loss =  -211.48822964438367  diff_norm =  0.006196960830673378\n",
      "iter  200  log loss =  -210.0270357937072  diff_norm =  0.004897254129765001\n",
      "iter  300  log loss =  -208.94836770669966  diff_norm =  0.004449606026607\n",
      "iter  400  log loss =  -208.01924310446873  diff_norm =  0.0041909897985166625\n",
      "iter  500  log loss =  -207.18235825547768  diff_norm =  0.004001404538128939\n",
      "iter  600  log loss =  -206.41315362334146  diff_norm =  0.003850011511331668\n",
      "iter  700  log loss =  -205.69692013491044  diff_norm =  0.003724819999182387\n",
      "iter  800  log loss =  -205.023533557183  diff_norm =  0.003618977801235843\n",
      "iter  900  log loss =  -204.38567654866637  diff_norm =  0.003527734552859317\n",
      "iter  1000  log loss =  -203.77795016893953  diff_norm =  0.0034475923142076904\n",
      "iter  1100  log loss =  -203.19631501058086  diff_norm =  0.003375942476506143\n",
      "iter  1200  log loss =  -202.6377074442413  diff_norm =  0.0033108392788276293\n",
      "iter  1300  log loss =  -202.09976863392185  diff_norm =  0.003250834102700098\n",
      "iter  1400  log loss =  -201.5806514179266  diff_norm =  0.0031948497770827684\n",
      "iter  1500  log loss =  -201.0788821425166  diff_norm =  0.0031420853011196884\n",
      "iter  1600  log loss =  -200.59326155616847  diff_norm =  0.003091944190940348\n",
      "iter  1700  log loss =  -200.12279355223387  diff_norm =  0.003043980999603075\n",
      "accuracy:  0.42857142857142855\n",
      "confusion Matrix:\n",
      " [[ 9. 16.]\n",
      " [ 4.  6.]]\n",
      "precision:  0.6\n",
      "recall:  0.2727272727272727\n",
      "\n",
      "==================\n",
      "classes are:  {'opel': -1, 'van': 1}\n",
      "iter  0  log loss =  -198.61293180836512  diff_norm =  0.2803109038971067\n",
      "iter  100  log loss =  -139.28471393036597  diff_norm =  0.01991647872911499\n",
      "iter  200  log loss =  -126.9433838403311  diff_norm =  0.012792173000178837\n",
      "iter  300  log loss =  -120.50359237518802  diff_norm =  0.010238512538999664\n",
      "iter  400  log loss =  -115.9557263019736  diff_norm =  0.00896323035474497\n",
      "iter  500  log loss =  -112.30272402802036  diff_norm =  0.00819248790980611\n",
      "iter  600  log loss =  -109.1737458384451  diff_norm =  0.007662197067365781\n",
      "iter  700  log loss =  -106.39744578627456  diff_norm =  0.007261228439206419\n",
      "iter  800  log loss =  -103.88252322996271  diff_norm =  0.0069366123927328644\n",
      "iter  900  log loss =  -101.57471388832104  diff_norm =  0.006660886391096922\n",
      "iter  1000  log loss =  -99.43871270450104  diff_norm =  0.006418843750285234\n",
      "iter  1100  log loss =  -97.44974456646078  diff_norm =  0.006201557804314733\n",
      "iter  1200  log loss =  -95.58931990181682  diff_norm =  0.006003486470157472\n",
      "iter  1300  log loss =  -93.84295868367568  diff_norm =  0.005820993775697135\n",
      "iter  1400  log loss =  -92.19889914499448  diff_norm =  0.005651563072993977\n",
      "iter  1500  log loss =  -90.64732469996719  diff_norm =  0.005493363341483009\n",
      "iter  1600  log loss =  -89.17987617390958  diff_norm =  0.005345002226659291\n",
      "iter  1700  log loss =  -87.78932804659206  diff_norm =  0.005205380798765653\n",
      "iter  1800  log loss =  -86.46936314058613  diff_norm =  0.005073605202217665\n",
      "iter  1900  log loss =  -85.21440904089351  diff_norm =  0.004948930939469269\n",
      "iter  2000  log loss =  -84.01951495189634  diff_norm =  0.004830726368669533\n",
      "iter  2100  log loss =  -82.88025618274148  diff_norm =  0.004718447835409147\n",
      "iter  2200  log loss =  -81.79265825844459  diff_norm =  0.004611622069503369\n",
      "iter  2300  log loss =  -80.75313545928213  diff_norm =  0.004509833273503474\n",
      "iter  2400  log loss =  -79.75844028146167  diff_norm =  0.004412713350599184\n",
      "iter  2500  log loss =  -78.80562136575904  diff_norm =  0.0043199343099766165\n",
      "iter  2600  log loss =  -77.89198812112188  diff_norm =  0.004231202235312519\n",
      "iter  2700  log loss =  -77.01508072522353  diff_norm =  0.004146252411110105\n",
      "iter  2800  log loss =  -76.1726444987161  diff_norm =  0.0040648453302495205\n",
      "iter  2900  log loss =  -75.36260787454208  diff_norm =  0.00398676338752907\n",
      "iter  3000  log loss =  -74.58306334838504  diff_norm =  0.003911808117062676\n",
      "iter  3100  log loss =  -73.8322509199718  diff_norm =  0.003839797867100837\n",
      "iter  3200  log loss =  -73.10854362953854  diff_norm =  0.0037705658307231786\n",
      "iter  3300  log loss =  -72.41043486730206  diff_norm =  0.0037039583686455904\n",
      "iter  3400  log loss =  -71.73652719166537  diff_norm =  0.0036398335735628186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  3500  log loss =  -71.08552243794233  diff_norm =  0.0035780600353828573\n",
      "iter  3600  log loss =  -70.45621293634018  diff_norm =  0.0035185157744346655\n",
      "iter  3700  log loss =  -69.84747368781518  diff_norm =  0.0034610873156820706\n",
      "iter  3800  log loss =  -69.25825537071678  diff_norm =  0.0034056688818820914\n",
      "iter  3900  log loss =  -68.68757807102605  diff_norm =  0.003352161687391417\n",
      "iter  4000  log loss =  -68.13452564533554  diff_norm =  0.003300473317487548\n",
      "iter  4100  log loss =  -67.59824063924034  diff_norm =  0.0032505171806276514\n",
      "iter  4200  log loss =  -67.07791969501585  diff_norm =  0.003202212023109034\n",
      "iter  4300  log loss =  -66.5728093918167  diff_norm =  0.003155481497351569\n",
      "iter  4400  log loss =  -66.08220246945604  diff_norm =  0.003110253776395492\n",
      "iter  4500  log loss =  -65.60543439341109  diff_norm =  0.003066461208417863\n",
      "iter  4600  log loss =  -65.14188022425782  diff_norm =  0.003024040005995416\n",
      "accuracy:  0.9393939393939394\n",
      "confusion Matrix:\n",
      " [[17.  0.]\n",
      " [ 2. 14.]]\n",
      "precision:  0.875\n",
      "recall:  1.0\n",
      "\n",
      "==================\n",
      "classes are:  {'bus': -1, 'opel': 1}\n",
      "iter  0  log loss =  -217.25559091631888  diff_norm =  0.23425174610747335\n",
      "iter  100  log loss =  -184.26673090022445  diff_norm =  0.017918719420764236\n",
      "iter  200  log loss =  -172.68945292860067  diff_norm =  0.013304143903796929\n",
      "iter  300  log loss =  -165.18399838767056  diff_norm =  0.011415916948285771\n",
      "iter  400  log loss =  -159.3188795026337  diff_norm =  0.010331260929782862\n",
      "iter  500  log loss =  -154.38624547441668  diff_norm =  0.009577240917254105\n",
      "iter  600  log loss =  -150.08851826020972  diff_norm =  0.008991636650972657\n",
      "iter  700  log loss =  -146.2692384778927  diff_norm =  0.008506541832109984\n",
      "iter  800  log loss =  -142.8323592839567  diff_norm =  0.008089068501555234\n",
      "iter  900  log loss =  -139.71228948511404  diff_norm =  0.007721115082939337\n",
      "iter  1000  log loss =  -136.8609405701266  diff_norm =  0.007391573473577913\n",
      "iter  1100  log loss =  -134.24133716367322  diff_norm =  0.007093029074394086\n",
      "iter  1200  log loss =  -131.82408524324356  diff_norm =  0.006820221846155859\n",
      "iter  1300  log loss =  -129.58523423276404  diff_norm =  0.0065692530180385955\n",
      "iter  1400  log loss =  -127.50489205891023  diff_norm =  0.00633713591610388\n",
      "iter  1500  log loss =  -125.56628244539337  diff_norm =  0.006121521169537315\n",
      "iter  1600  log loss =  -123.75507894778738  diff_norm =  0.0059205186561703885\n",
      "iter  1700  log loss =  -122.05892040153438  diff_norm =  0.005732577389115129\n",
      "iter  1800  log loss =  -120.46704948506085  diff_norm =  0.00555640216610633\n",
      "iter  1900  log loss =  -118.97003714907399  diff_norm =  0.0053908945000860275\n",
      "iter  2000  log loss =  -117.55956833426949  diff_norm =  0.005235110030253631\n",
      "iter  2100  log loss =  -116.2282723537455  diff_norm =  0.005088227333399629\n",
      "iter  2200  log loss =  -114.96958646315798  diff_norm =  0.004949524734968607\n",
      "iter  2300  log loss =  -113.77764454848797  diff_norm =  0.004818362801373149\n",
      "iter  2400  log loss =  -112.64718515787543  diff_norm =  0.004694170912314398\n",
      "iter  2500  log loss =  -111.57347467681981  diff_norm =  0.004576436796232252\n",
      "iter  2600  log loss =  -110.55224253920329  diff_norm =  0.004464698243277214\n",
      "iter  2700  log loss =  -109.5796261374009  diff_norm =  0.004358536438672145\n",
      "iter  2800  log loss =  -108.6521236461702  diff_norm =  0.00425757051816746\n",
      "iter  2900  log loss =  -107.766553375359  diff_norm =  0.004161453058284009\n",
      "iter  3000  log loss =  -106.92001856147104  diff_norm =  0.004069866292139221\n",
      "iter  3100  log loss =  -106.10987672877803  diff_norm =  0.003982518896871191\n",
      "iter  3200  log loss =  -105.3337129181516  diff_norm =  0.0038991432380599137\n",
      "iter  3300  log loss =  -104.589316210744  diff_norm =  0.0038194929847573614\n",
      "iter  3400  log loss =  -103.87465907431024  diff_norm =  0.0037433410291717564\n",
      "iter  3500  log loss =  -103.18787913955418  diff_norm =  0.0036704776599480364\n",
      "iter  3600  log loss =  -102.52726307755609  diff_norm =  0.003600708948963301\n",
      "iter  3700  log loss =  -101.89123230084554  diff_norm =  0.0035338553197583893\n",
      "iter  3800  log loss =  -101.27833025274468  diff_norm =  0.003469750271840927\n",
      "iter  3900  log loss =  -100.68721108426476  diff_norm =  0.003408239239863815\n",
      "iter  4000  log loss =  -100.1166295466117  diff_norm =  0.0033491785702548585\n",
      "iter  4100  log loss =  -99.56543195141347  diff_norm =  0.003292434600752592\n",
      "iter  4200  log loss =  -99.03254807101162  diff_norm =  0.003237882830561704\n",
      "iter  4300  log loss =  -98.51698386826999  diff_norm =  0.0031854071706362685\n",
      "iter  4400  log loss =  -98.01781495988308  diff_norm =  0.003134899265074599\n",
      "iter  4500  log loss =  -97.53418072957027  diff_norm =  0.0030862578758559674\n",
      "iter  4600  log loss =  -97.06527901815574  diff_norm =  0.003039388324120506\n",
      "accuracy:  0.9142857142857143\n",
      "confusion Matrix:\n",
      " [[16.  3.]\n",
      " [ 0. 16.]]\n",
      "precision:  1.0\n",
      "recall:  0.8421052631578947\n",
      "\n",
      "==================\n",
      "classes are:  {'bus': -1, 'van': 1}\n",
      "iter  0  log loss =  -203.78588869939128  diff_norm =  0.289032350609874\n",
      "iter  100  log loss =  -162.74959804218958  diff_norm =  0.02051258523608678\n",
      "iter  200  log loss =  -147.71252435351892  diff_norm =  0.015008800824577397\n",
      "iter  300  log loss =  -138.36144515100764  diff_norm =  0.012608822654178063\n",
      "iter  400  log loss =  -131.3145520682249  diff_norm =  0.011248019334624305\n",
      "iter  500  log loss =  -125.52187430457435  diff_norm =  0.01033736155229995\n",
      "iter  600  log loss =  -120.54183778001122  diff_norm =  0.009657229154626887\n",
      "iter  700  log loss =  -116.1496030788902  diff_norm =  0.00911075578273677\n",
      "iter  800  log loss =  -112.213976944079  diff_norm =  0.0086498062759851\n",
      "iter  900  log loss =  -108.65011535354415  diff_norm =  0.008248169937728686\n",
      "iter  1000  log loss =  -105.39862937583133  diff_norm =  0.007890469291468324\n",
      "iter  1100  log loss =  -102.41534348616256  diff_norm =  0.007567085711599378\n",
      "iter  1200  log loss =  -99.66584621957709  diff_norm =  0.007271648564642303\n",
      "iter  1300  log loss =  -97.12238637539924  diff_norm =  0.006999712803072676\n",
      "iter  1400  log loss =  -94.76199758687446  diff_norm =  0.006748026301428241\n",
      "iter  1500  log loss =  -92.56530350228095  diff_norm =  0.006514106511628658\n",
      "iter  1600  log loss =  -90.51571896266334  diff_norm =  0.006295986920684675\n",
      "iter  1700  log loss =  -88.59889193916428  diff_norm =  0.006092060316666053\n",
      "iter  1800  log loss =  -86.80229795415828  diff_norm =  0.005900979029884966\n",
      "iter  1900  log loss =  -85.11493489424006  diff_norm =  0.005721589629817452\n",
      "iter  2000  log loss =  -83.52708641062921  diff_norm =  0.005552888961672926\n",
      "iter  2100  log loss =  -82.03013385081093  diff_norm =  0.0053939936886120395\n",
      "iter  2200  log loss =  -80.61640366942198  diff_norm =  0.0052441185588792535\n",
      "iter  2300  log loss =  -79.27904155690364  diff_norm =  0.005102560425443825\n",
      "iter  2400  log loss =  -78.01190722203988  diff_norm =  0.004968686139155041\n",
      "iter  2500  log loss =  -76.80948550485219  diff_norm =  0.004841923109208166\n",
      "iter  2600  log loss =  -75.66681064862745  diff_norm =  0.004721751744968325\n",
      "iter  2700  log loss =  -74.57940134313577  diff_norm =  0.004607699259279761\n",
      "iter  2800  log loss =  -73.54320469782026  diff_norm =  0.004499334483761991\n",
      "iter  2900  log loss =  -72.55454769549051  diff_norm =  0.004396263456968591\n",
      "iter  3000  log loss =  -71.61009496497805  diff_norm =  0.004298125618537817\n",
      "iter  3100  log loss =  -70.70681192794123  diff_norm =  0.004204590490359852\n",
      "iter  3200  log loss =  -69.84193254174808  diff_norm =  0.0041153547578241835\n",
      "iter  3300  log loss =  -69.01293099116295  diff_norm =  0.004030139686071223\n",
      "iter  3400  log loss =  -68.21749678590294  diff_norm =  0.003948688821207455\n",
      "iter  3500  log loss =  -67.45351280556933  diff_norm =  0.0038707659370800637\n",
      "iter  3600  log loss =  -66.7190359026069  diff_norm =  0.003796153195796359\n",
      "iter  3700  log loss =  -66.01227973114139  diff_norm =  0.003724649495800128\n",
      "iter  3800  log loss =  -65.33159951723822  diff_norm =  0.0036560689855188663\n",
      "iter  3900  log loss =  -64.67547852615904  diff_norm =  0.003590239723872806\n",
      "iter  4000  log loss =  -64.04251601599181  diff_norm =  0.0035270024715243985\n",
      "iter  4100  log loss =  -63.43141649568129  diff_norm =  0.0034662095987953185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  4200  log loss =  -62.84098012988635  diff_norm =  0.0034077240979970497\n",
      "iter  4300  log loss =  -62.270094153922216  diff_norm =  0.00335141868928331\n",
      "iter  4400  log loss =  -61.71772517989027  diff_norm =  0.0032971750104281\n",
      "iter  4500  log loss =  -61.182912290421235  diff_norm =  0.0032448828820092223\n",
      "iter  4600  log loss =  -60.66476082964479  diff_norm =  0.003194439640375195\n",
      "iter  4700  log loss =  -60.16243681236919  diff_norm =  0.0031457495316400633\n",
      "iter  4800  log loss =  -59.67516188228142  diff_norm =  0.0030987231606250955\n",
      "iter  4900  log loss =  -59.20220875848467  diff_norm =  0.003053276989348623\n",
      "iter  5000  log loss =  -58.74289711706461  diff_norm =  0.003009332880206195\n",
      "accuracy:  1.0\n",
      "confusion Matrix:\n",
      " [[15.  0.]\n",
      " [ 0. 18.]]\n",
      "precision:  1.0\n",
      "recall:  1.0\n",
      "\n",
      "==================\n",
      "classes are:  {'saab': -1, 'van': 1}\n",
      "iter  0  log loss =  -202.24850896815508  diff_norm =  0.30523699101744817\n",
      "iter  100  log loss =  -143.18693430170367  diff_norm =  0.020808136878603212\n",
      "iter  200  log loss =  -129.20646211289335  diff_norm =  0.013881825236444286\n",
      "iter  300  log loss =  -121.46000791007394  diff_norm =  0.011335709230705393\n",
      "iter  400  log loss =  -115.82809746668913  diff_norm =  0.01001384329668078\n",
      "iter  500  log loss =  -111.25302820997652  diff_norm =  0.009177253046203596\n",
      "iter  600  log loss =  -107.3277924495439  diff_norm =  0.008577226772801401\n",
      "iter  700  log loss =  -103.85649203166847  diff_norm =  0.008108714423733279\n",
      "iter  800  log loss =  -100.73027927766448  diff_norm =  0.007720632858392745\n",
      "iter  900  log loss =  -97.88193470361657  diff_norm =  0.007385766184529652\n",
      "iter  1000  log loss =  -95.26627292570187  diff_norm =  0.007088630046931942\n",
      "iter  1100  log loss =  -92.85072444931244  diff_norm =  0.006819926325600019\n",
      "iter  1200  log loss =  -90.61044898877009  diff_norm =  0.006573785937913187\n",
      "iter  1300  log loss =  -88.5256452624342  diff_norm =  0.006346315776640044\n",
      "iter  1400  log loss =  -86.5799927572824  diff_norm =  0.006134800290810347\n",
      "iter  1500  log loss =  -84.75970505925764  diff_norm =  0.0059372487162421585\n",
      "iter  1600  log loss =  -83.05292684438942  diff_norm =  0.005752131705478082\n",
      "iter  1700  log loss =  -81.44933102091274  diff_norm =  0.005578224681561303\n",
      "iter  1800  log loss =  -79.9398366315732  diff_norm =  0.005414512649544184\n",
      "iter  1900  log loss =  -78.51640234985506  diff_norm =  0.0052601310284412185\n",
      "iter  2000  log loss =  -77.17186918704482  diff_norm =  0.005114327920488127\n",
      "iter  2100  log loss =  -75.89983657781222  diff_norm =  0.004976439325743052\n",
      "iter  2200  log loss =  -74.69456206262696  diff_norm =  0.004845872294418447\n",
      "iter  2300  log loss =  -73.55087832562221  diff_norm =  0.004722093032101867\n",
      "iter  2400  log loss =  -72.46412346205182  diff_norm =  0.004604618160696679\n",
      "iter  2500  log loss =  -71.43008164354455  diff_norm =  0.004493008041646966\n",
      "iter  2600  log loss =  -70.44493216232746  diff_norm =  0.004386861488224312\n",
      "iter  2700  log loss =  -69.50520536204124  diff_norm =  0.00428581144628622\n",
      "iter  2800  log loss =  -68.60774431530525  diff_norm =  0.004189521375960744\n",
      "iter  2900  log loss =  -67.7496713527727  diff_norm =  0.004097682160180013\n",
      "iter  3000  log loss =  -66.92835872424735  diff_norm =  0.004010009423595996\n",
      "iter  3100  log loss =  -66.14140280313504  diff_norm =  0.003926241181478779\n",
      "iter  3200  log loss =  -65.38660134562836  diff_norm =  0.0038461357610483442\n",
      "iter  3300  log loss =  -64.66193339470087  diff_norm =  0.0037694699525291416\n",
      "iter  3400  log loss =  -63.96554148212083  diff_norm =  0.0036960373571140936\n",
      "iter  3500  log loss =  -63.29571583319437  diff_norm =  0.003625646905771685\n",
      "iter  3600  log loss =  -62.6508803215209  diff_norm =  0.003558121527641819\n",
      "iter  3700  log loss =  -62.029579956566835  diff_norm =  0.0034932969503193257\n",
      "iter  3800  log loss =  -61.43046971676567  diff_norm =  0.0034310206169584742\n",
      "iter  3900  log loss =  -60.85230456614687  diff_norm =  0.0033711507072982988\n",
      "iter  4000  log loss =  -60.293930514019415  diff_norm =  0.0033135552513731043\n",
      "iter  4100  log loss =  -59.7542765956113  diff_norm =  0.003258111326101661\n",
      "iter  4200  log loss =  -59.232347667307714  diff_norm =  0.0032047043261058668\n",
      "iter  4300  log loss =  -58.72721792366014  diff_norm =  0.0031532273011585527\n",
      "iter  4400  log loss =  -58.23802505498671  diff_norm =  0.0031035803534946497\n",
      "iter  4500  log loss =  -57.76396497444671  diff_norm =  0.003055670088979009\n",
      "iter  4600  log loss =  -57.30428705217119  diff_norm =  0.0030094091168261054\n",
      "accuracy:  0.9705882352941176\n",
      "confusion Matrix:\n",
      " [[18.  0.]\n",
      " [ 1. 15.]]\n",
      "precision:  0.9375\n",
      "recall:  1.0\n"
     ]
    }
   ],
   "source": [
    "arrSize = len(oneVsOneMaps)\n",
    "data_arr = np.zeros(arrSize, dtype = \"object\")\n",
    "label_arr = np.zeros(arrSize, dtype = \"object\")\n",
    "x_train_arr = np.zeros(arrSize, dtype = \"object\")\n",
    "y_train_arr = np.zeros(arrSize, dtype = \"object\")\n",
    "x_test_arr = np.zeros(arrSize, dtype = \"object\")\n",
    "y_test_arr = np.zeros(arrSize, dtype = \"object\")\n",
    "w_res_arr = np.zeros(arrSize, dtype = \"object\")\n",
    "x_train_total, x_test_total, y_train_total, y_test_total = train_test_split(allData, dataset[:,-1], test_size=0.2)\n",
    "\n",
    "\n",
    "for i in range(arrSize):\n",
    "    print(\"\\n==================\")\n",
    "    print(\"classes are: \", oneVsOneMaps[i])\n",
    "    data_arr[i], label_arr[i] = convertForOneToOne(x_train_total, y_train_total, oneVsOneMaps[i])\n",
    "    x_train_arr[i], x_test_arr[i], y_train_arr[i], y_test_arr[i] = train_test_split(data_arr[i], label_arr[i], test_size=0.1)\n",
    "    w_res_arr[i] = logReg(x_train_arr[i],y_train_arr[i], 0.003, 0.002, 0.5, False)\n",
    "    result = replaceOnLabels(x_test_arr[i].dot(w_res_arr[i]))\n",
    "    print(\"accuracy: \", accuracy(result, y_test_arr[i]))\n",
    "    classValues = np.fromiter(oneVsOneMaps[i].values(), dtype=int)\n",
    "    confusion_matrix = confusionMatrix(result,y_test_arr[i], classValues)\n",
    "    print(\"confusion Matrix:\\n\",  confusion_matrix)\n",
    "    print(\"precision: \", precision(confusion_matrix, classValues[0]))\n",
    "    print(\"recall: \", recall(confusion_matrix, classValues[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def getInvMap(inputMap):\n",
    "    return {v: k for k, v in inputMap.items()}\n",
    "\n",
    "def getPredictedClass(prediction, twoMap):\n",
    "    invMap = getInvMap(twoMap)\n",
    "    if (prediction < 0):\n",
    "        return invMap.get(-1)\n",
    "    else:\n",
    "        return invMap.get(1)\n",
    "    \n",
    "def predictOneToOne(w_arr, maps, x_test, classes):\n",
    "    classProbability = x_test.dot(w_arr[0])\n",
    "    result = np.zeros(x_test.shape[0], dtype = \"object\")\n",
    "    for i in range(1, len(maps)):\n",
    "        classProbability = np.vstack((classProbability, x_test.dot(w_arr[i])))\n",
    "    classProbability = classProbability.T\n",
    "    print(classProbability.shape)\n",
    "    for i in range(len(result)):\n",
    "        predictionDist = {k : 0. for k in classes}\n",
    "        for j in range(len(maps)):\n",
    "            probability = sigmoid(abs(classProbability[i][j]))                \n",
    "            predictedClass = getPredictedClass(classProbability[i][j], maps[j])\n",
    "            predictionDist[predictedClass] = predictionDist[predictedClass] + probability\n",
    "#         print(\"predictionDist: \", predictionDist)\n",
    "        finalPrediction = max(predictionDist.items(), key=operator.itemgetter(1))[0]\n",
    "#         print(\"finalPrediction: \", finalPrediction)\n",
    "        result[i] = finalPrediction\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170, 6)\n",
      "accuracy =  0.6294117647058823\n",
      "distribution of classes in the sample \n",
      "\tclass  bus  :  43.0\n",
      "\tclass  saab  :  41.0\n",
      "\tclass  opel  :  42.0\n",
      "\tclass  van  :  44.0\n",
      "confusion matrix for classes  ['bus', 'saab', 'opel', 'van']  is:\n",
      " [[27.  1.  1.  1.]\n",
      " [ 1.  2.  0.  0.]\n",
      " [ 6. 27. 35.  0.]\n",
      " [ 9. 11.  6. 43.]]\n",
      "for class  bus\n",
      "precision :  0.9\n",
      "recall :  0.627906976744186\n",
      "for class  saab\n",
      "precision :  0.6666666666666666\n",
      "recall :  0.04878048780487805\n",
      "for class  opel\n",
      "precision :  0.5147058823529411\n",
      "recall :  0.8333333333333334\n",
      "for class  van\n",
      "precision :  0.6231884057971014\n",
      "recall :  0.9772727272727273\n"
     ]
    }
   ],
   "source": [
    "oneVsOneResult = predictOneToOne(w_res_arr, oneVsOneMaps, x_test_total,classes)\n",
    "print(\"accuracy = \", accuracy(oneVsOneResult, y_test_total))\n",
    "print(\"distribution of classes in the sample \")\n",
    "calculateClassAm(classes, y_test_total)\n",
    "final_oneVsOne_confusionMatrix = confusionMatrix(oneVsOneResult, y_test_total, classes)\n",
    "print(\"confusion matrix for classes \", classes, \" is:\\n\", final_oneVsOne_confusionMatrix)\n",
    "for i in range(len(classes)):\n",
    "    print(\"for class \", classes[i])\n",
    "    print(\"precision : \", precision(final_oneVsOne_confusionMatrix, i))\n",
    "    print(\"recall : \", recall(final_oneVsOne_confusionMatrix, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
